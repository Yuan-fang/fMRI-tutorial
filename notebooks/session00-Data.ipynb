{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241af88d",
   "metadata": {},
   "source": [
    "# Session 0: Start with fMRI data\n",
    "\n",
    "In this session, we will learn some basics about fMRI data, including fMRI data visualization & access.\n",
    "\n",
    "## Tools We’ll Use\n",
    "\n",
    "### **Nilearn**\n",
    "In this tutorial, most of the analyses will be carried out with **[Nilearn](https://nilearn.github.io/stable/index.html)**.  Nilearn is a Python library that simplifies the use of machine learning and statistical tools for fMRI data. It provides convenient functions to load, manipulate, and visualize brain images, as well as to build predictive models and perform decoding analyses.\n",
    "\n",
    "We will use Nilearn to perform both:\n",
    "- **First-level analyses**, which model individual subjects’ brain responses to experimental events and produce subject-specific activation maps.\n",
    "- **Second-level analyses**, which combine these individual results across participants to identify reliable group-level activation patterns.\n",
    "\n",
    "Beyond GLM-based activation analyses, Nilearn also supports:\n",
    "- Functional connectivity and network analyses  \n",
    "- Decoding and multivariate pattern analysis (MVPA)  \n",
    "- High-quality visualization in both 2D and 3D brain spaces  \n",
    "\n",
    "## Running Environment: Neurodesktop\n",
    "\n",
    "One big hassel for fMRI data analysis is that you need to install and set many softwares locally or in a server. While you can install them by yourself after class at your wish, for this tutorial, we'd like to save your time and efforts from installing various neuroimaging softwares (there are tons of softwares if you count them!). \n",
    "\n",
    "For this reason, we will run notebooks on **[Neurodesktop](https://neurodesk.org/)** — a software container with major neuroimaging tools, including FSL, AFNI, ANTs, and fMRIPrep preinstalled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic Setup (always run this first) ---\n",
    "\n",
    "# Install dependencies \n",
    "%pip install -q gdown\n",
    "%pip install -q git+https://github.com/Yuan-fang/fMRI-tutorial.git\n",
    "\n",
    "# Import essential packages\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import module\n",
    "from nilearn import image, plotting\n",
    "from nilearn.image import index_img\n",
    "from bids import BIDSLayout\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tutorial.utils.paths import PathManager\n",
    "from tutorial.utils.fetch import fetch_dataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3638080",
   "metadata": {},
   "source": [
    "We also need to set up data directories. You can change to other directories and names according to your preference. But for this tutorial, let's stick with the same data directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up data directories ---\n",
    "\n",
    "DATASET = \"Haxby2001\" # name of the dataset\n",
    "BASE_DIR = Path.home() / \"fmri_tutorial\" # base directory for the tutorial\n",
    "DATA_DIR = BASE_DIR / \"data\" / DATASET # data directory for the dataset\n",
    "DERIV_DIR = BASE_DIR / \"data\" / \"derivatives\" # derivatives directory for processed data\n",
    "\n",
    "for p in (DATA_DIR, DERIV_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset if not already present ---\n",
    "\n",
    "# Google Drive link to the dataset\n",
    "download_url = \"https://drive.google.com/uc?id=1fPjbWhY6ZDOGSm59duKmOcCpgp5Zf5tX\"\n",
    "fetch_dataset(download_url, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57399862",
   "metadata": {},
   "source": [
    "### fMRI data folder: BIDS\n",
    "\n",
    "Let's first familiarize yourself with the structure of fMI data. As you can see from the \"./data\" folder, the data is structured in BIDS format.\n",
    "\n",
    "BIDS (Brain Imaging Data Structure) is a data structure standard many researchers follow. As different people follow the same structure and naming convention to organize their data, it will allow the same set of pipelines to process different people's data with convenience (see more info about BIDS: https://bids.neuroimaging.io/index.html).\n",
    "\n",
    "Let's quickly browse our data. One easy-to-think way is to kick and click the specific data folder to see what's inside. Here we use the package pyBIDS (https://bids-standard.github.io/pybids/user_guide.html) to help us *inferacte* the data with commmandlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48976cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BIDSLayout object using BIDSLayout (https://bids-standard.github.io/pybids/generated/bids.layout.BIDSLayout.html)\n",
    "# BIDSLayout is a tool from the package pybids that will help us index our dataset. \n",
    "# By creating a BIDSLayout object, we can easily query the dataset to get file paths of specific files.\n",
    "layout = BIDSLayout(DATA_DIR, validate=False) # we set validate=False to skip BIDS validation for speed. Other parameters can be found in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e32ca55",
   "metadata": {},
   "source": [
    "Let's explore what's inside this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccf57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all subjects in the dataset\n",
    "subjects = layout.get_subjects()\n",
    "print(\"Subjects:\", subjects)\n",
    "\n",
    "# List all data types available in the dataset for a specific subject.\n",
    "# Data types can be anat (anatomical), func (functional), dwi (diffusion), etc.\n",
    "# You can change the subject ID and session\n",
    "datatypes = layout.get_datatypes(subject=\"1\")\n",
    "print(\"Data types:\", datatypes)\n",
    "\n",
    "# For functional data, we often have multiple runs. \n",
    "# Let's list all functional runs for a specific subject\n",
    "runs = layout.get_runs(subject=\"1\")\n",
    "print(\"Runs for subject 1:\", runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d52dd7",
   "metadata": {},
   "source": [
    "With the BIDSLayout object, we can not only browse the data at ease, but also get access to the path of specific data conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1269a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file path of anatomical image for a specific subject\n",
    "# .nii.gz is the common file extension for NIfTI files, which is a standard format for storing neuroimaging data\n",
    "anat_filepath = layout.get(subject=\"1\", suffix=\"T1w\", extension=\".nii.gz\", return_type=\"file\")[0] # note that we use [0] to get the first item in the list\n",
    "print(\"Anatomical files for subject 1:\", anat_filepath)\n",
    "\n",
    "# Get the file path of functional image in run 1 for a specific subject\n",
    "func_filepath = layout.get(subject=\"1\", suffix=\"bold\", extension=\".nii.gz\", run=\"1\", return_type=\"file\")[0]\n",
    "print(\"Functional files for subject 1, run 1:\", func_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a0845",
   "metadata": {},
   "source": [
    "### Visualize the anatomical image\n",
    "\n",
    "There are many other aspects about the BIDS structure you can explore with the BIDSLayout object you've just created. I will leave them to you to explore after the class. \n",
    "\n",
    "Here, let's first focus on the anatomical image, i.e., T1w (T1-weighted) image. We will visualize the T1w image.\n",
    "\n",
    "For visualization and many further fMRI analyses, we will rely on the package Nilearn (https://nilearn.github.io/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f822d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot this subject's anatomical image at the default coordinates\n",
    "view = plotting.plot_anat(anat_filepath) # see https://nilearn.github.io/stable/modules/generated/nilearn.plotting.plot_anat.html for more options\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f197b",
   "metadata": {},
   "source": [
    "You can see from the above that the default display show three views (rememeber the name of each view?) at one specific location (7, -3, 7). The coordinates are in millimeter unit. The coordinates share similiar meaning with MNI coordinates. However, as the T1w image is not yet normalized to a template space (such as MNI space), we just call them \"coordinates in the local/native space\".\n",
    "\n",
    "Now let's look at another location in the local space, e.g., (-30, 10, 40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a092e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the cut coordinates to (-30, 10, 40) \n",
    "view = plotting.plot_anat(anat_filepath, cut_coords=(-30, 10, 40))\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191cab8",
   "metadata": {},
   "source": [
    "You must have noticed taht the above plotting method only allows you to visualize images at a specific location. \n",
    "\n",
    "To explore the T1w image more interactively, you can use another more general-purpose tool in Nilearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a90b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To explore the image interactively, you could use another methods.\n",
    "plotting.view_img(anat_filepath, cmap=\"gray\") # see https://nilearn.github.io/stable/modules/generated/nilearn.plotting.view_img.html for more options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ebb6a3",
   "metadata": {},
   "source": [
    "### Visualize the functional image\n",
    "\n",
    "After visualizing the T1w image. Now let's visualize one run from the func images.\n",
    "\n",
    "Different from the T1w image in 3D, for func images, they are 4D. For Nilearn, plotting 4D data is difficult (actually Nilearn's interactive data viewing functions overally is far from comparable to other softwares, such as FSL's fslview). Therefore, we need to pick one volume from the concatenated 4D volumes to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab102dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index of the first volume from the functional image (you can change this index to plot other volumes)\n",
    "volume_index = 0 # note that the index starts from 0\n",
    "# Get the volume with the specified index from the functional image\n",
    "# see https://nilearn.github.io/stable/modules/generated/nilearn.image.index_img.html for more options\n",
    "vol = image.index_img(func_filepath, volume_index)\n",
    "# Plot the selected volume with plot_epi.\n",
    "# plot_epi is a counterpart to plot_anat. see https://nilearn.github.io/stable/modules/generated/nilearn.plotting.plot_epi.html for more options\n",
    "plotting.plot_epi(vol, title=\"BOLD volume t=0\")\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9508011e",
   "metadata": {},
   "source": [
    "Did you notice that the functional images come at much lower resolution than the T1w (T1-weighted) structural image?\n",
    "\n",
    "In the next section, we will see how to get their seperated resolution information from the images themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c307344",
   "metadata": {},
   "source": [
    "#### 🧠 Do it yourself: \n",
    "Can you display the above volume at another location, say, (-30, 20, 11), with *plot_epi*?\n",
    "\n",
    "_Tpye your answer in the cell below. then check the answer._\n",
    "\n",
    "<details>\n",
    "<summary>💡 Show the correct answer</summary>\n",
    "\n",
    "````python\n",
    "view = plotting.plot_epi(vol, title=\"BOLD volume t=0\", cut_coords=(-30, 20, 11))\n",
    "plotting.show()\n",
    "````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65330059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and execute your code below to display the above volume at different coordinates (-30, 20, 11)\n",
    "# --- YOUR CODE HERE ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c65f47",
   "metadata": {},
   "source": [
    "#### 🧠 Do it yourself: \n",
    "Similiar to T1w image, we can also display the volume interactively. Remeber the more general-purpose tool *view_img*?\n",
    "\n",
    "_Tpye your answer in the cell below. then check the answer._\n",
    "\n",
    "<details>\n",
    "<summary>💡 Show the correct answer</summary>\n",
    "\n",
    "````python\n",
    "plotting.view_img(vol, cmap=\"gray\") \n",
    "````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and execute your code below to display the above volume interactively\n",
    "# --- YOUR CODE HERE ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b502785",
   "metadata": {},
   "source": [
    "In Nilearn, although the ways to display 4D data is very limited, we do have one way to visualize 4D func images. But maybe different from what you expect, this method plot the image of voxel intensities across time. So the 3D data in each volume is flattened to 1D.\n",
    "\n",
    "This method is very useful for a quick glimpse of the headmovement noise in a func run. Run the code cell below. Do you notice several bands (abrupt change in overall grayscale in nearby time)? They are often related to head movement induced global (whole brain) signal fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ba909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the functional (bold) images\n",
    "# see https://nilearn.github.io/stable/modules/generated/nilearn.plotting.plot_carpet.html for more options\n",
    "plotting.plot_carpet(func_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d3b7b",
   "metadata": {},
   "source": [
    "### Access the fMRI data\n",
    "\n",
    "We just displayed the T1w and func images. Very often, we want to directly access the fMRI data, so that we can do some specific computations on them. To do this, we need to load the data as some 3D or 4D arrays.\n",
    "\n",
    "In the next, we first load the T1w image as a Nifti1Image object. Nifti1Image is just a Python object which contains as many information as you need about the T1w image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the T1w image\n",
    "# It reads the image from the file and stores it as a Nifti1Image object (https://nilearn.github.io/stable/modules/generated/nilearn.image.load_img.html)\n",
    "t1_img = image.load_img(anat_filepath)\n",
    "\n",
    "# Print the Nifti1Image object to see its information\n",
    "print(t1_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e918a8",
   "metadata": {},
   "source": [
    "You can see the Nifti1Image object holding all the information about the T1w image. We do not need to print all those information (It's too cluttered). Here we just want to know the image's shape and voxel size. So we can do something below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f17891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the T1w image (dimensions in x, y, z))\n",
    "print(\"Shape (x, y, z):\", t1_img.shape)\n",
    "\n",
    "# Print the voxel size (in mm) in each dimension (x, y, z)\n",
    "print(\"Voxel size (mm):\", t1_img.header.get_zooms())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b847d6",
   "metadata": {},
   "source": [
    "We can get access to the actual value (MRI signal, or grayscale intensity value) in a specific coordinate. You can really see the T1w data is essentially a 3D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12041a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image data as a numpy array from the Nifti1Image object\n",
    "t1_data = t1_img.get_fdata()\n",
    "\n",
    "# Print the shape and data type of the image data\n",
    "print(\"Data type:\", t1_data.dtype)\n",
    "print(\"Data shape:\", t1_data.shape)\n",
    "\n",
    "# Access the value at a specific voxel coordinate (x=90, y=120, z=130) in voxel space. \n",
    "# You can change the coordinate to any value within the image dimensions.\n",
    "# Note that the coordinate starts from (0, 0, 0)\n",
    "print(\"Value at a specific voxel coordinate:\", t1_data[90, 120, 130])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03056b10",
   "metadata": {},
   "source": [
    "Note in the above example, the coordinates (90, 120, 130) are voxel space coordinates. Or put it another way, they are just *index* for specific locations in the 3D data matrix (or array). So they starts from (0, 0, 0). The unit of the coordinate is *voxel*. They are different from the coordinates in the native space you saw earlier, such as (-30, 10, 40), the unit of which is *millimeter (mm)*. \n",
    "\n",
    "But what if we want to know the voxel space coordinates (90, 120, 130) correspond to what coordinaes in the native space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the affine matrix of the T1w image\n",
    "affine = t1_img.affine\n",
    "print(\"Affine matrix:\\n\", affine)\n",
    "\n",
    "# Get the coordinates in native space (in mm) corresponding to the voxel space coordinate (90, 120, 130)\n",
    "voxel_coord = [90, 120, 130, 1]  # add a 1 for homogeneous coordinates\n",
    "native_coord = affine.dot(voxel_coord)  # matrix multiplication\n",
    "print(\"Coordinates in native space (mm):\", native_coord[:3])  # exclude the last element\n",
    "\n",
    "# Another more convenient way to convert voxel space coordinates to native space coordinates is to use the function `nilearn.image.coord_transform`\n",
    "# see https://nilearn.github.io/stable/modules/generated/nilearn.image.coord_transform\n",
    "native_coord2 = image.coord_transform(90, 120, 130, affine)\n",
    "print(\"Coordinates in native space (mm) using coord_transform:\", native_coord2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64c112",
   "metadata": {},
   "source": [
    "As you can see above the above example, to convert voxel space coordinates to native space coordinates, or put it into another way, to find the correspondence between the voxel locations in an array to real-word coordinates, one piece of information is very important, they are `affine` matrix.\n",
    "\n",
    "`affine` is a **4×4 matrix** that maps from voxel indices `(i, j, k)`  to scanner-space (or native-space) coordinates `(x, y, z)` in **millimeters**.\n",
    "\n",
    "Mathematically, the mapping is:\n",
    "\n",
    "$$[x, y, z, 1]^T = \\text{affine} \\cdot [i, j, k, 1]^T$$\n",
    "\n",
    "\n",
    "\n",
    "At its core, fMRI data are remarkably simple. They consist of two key components:\n",
    "\n",
    "1. **A data matrix** — a 3D (or 4D) array of numbers. Conceptually, this is no different from any other dataset you might encounter — whether in agriculture, transportation, or finance. Each element in the matrix represents a data point (e.g., voxel intensity) at a specific index.\n",
    "\n",
    "2. **A linear transformation matrix (`affine`)** — this defines how the voxel indices in the data matrix correspond to real-world spatial locations in millimeters. In other words, it tells you *where* each data point lies in the brain or scanner space.\n",
    "\n",
    "Together, these two components allow us to move seamlessly between array indices and meaningful anatomical coordinates.\n",
    "\n",
    "\n",
    "Next, let's see what an func image (T2* image) looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8298866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the functional image\n",
    "func_img = image.load_img(func_filepath)\n",
    "\n",
    "# Print the Nifti1Image object to see its information\n",
    "print(func_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974624f",
   "metadata": {},
   "source": [
    "#### 🧠 Do it yourself: \n",
    "Similiar to T1w image, please print the shape of this func image and it's voxel size.\n",
    "\n",
    "_Type your answer in the cell below. then check the answer._\n",
    "\n",
    "<details>\n",
    "<summary>💡 Show the correct answer</summary>\n",
    "\n",
    "````python\n",
    "# Print the shape of the func image (dimensions in x, y, z, t))\n",
    "print(\"Shape (x, y, z, t):\", func_img.shape)\n",
    "\n",
    "# Print the voxel size (in mm) in each dimension (x, y, z)\n",
    "print(\"Voxel size (mm):\", func_img.header.get_zooms())\n",
    "\n",
    "\n",
    "````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ec499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and execute your code below to display the shape of this func image and it's voxel size.\n",
    "# --- YOUR CODE HERE ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c73f9",
   "metadata": {},
   "source": [
    "Did you notice that for func images, different from T1w image, the outputs of `func_img.shape` and `func_img.header.get_zooms()` has the 4th dimension? It is the temporal dimension, as we are dealing with the functional image.\n",
    "\n",
    "1. The 4th dimension in the output of `func_img.shape` tell you **how many volumes(TRs) in total for this func image** \n",
    "2. The 4th dimension in the output of `func_img.header.get_zooms()` tell you **the resolution in the temporal dimension (i.e, TR)** \n",
    "\n",
    "Now, let's extract one voxel's time course from the `func_img`. Let's say, the voxel is located in coordinates (0. -10, 30) (mm unit). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840997c6",
   "metadata": {},
   "source": [
    "#### 🧠 Do it yourself: \n",
    "Please first extract the actual data from `func_img`. Remember to print the data type and shape.\n",
    "\n",
    "_Type your answer in the cell below. then check the answer._\n",
    "\n",
    "<details>\n",
    "<summary>💡 Show the correct answer</summary>\n",
    "\n",
    "````python\n",
    "# Load the image data as a numpy array from the Nifti1Image object\n",
    "func_data = func_img.get_fdata()\n",
    "\n",
    "# Print the shape and data type of the image data\n",
    "print(\"Data type:\", func_data.dtype)\n",
    "print(\"Data shape:\", func_data.shape)\n",
    "\n",
    "\n",
    "````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and execute your code below to extract the 4D data from `func_img`. Remember to print the data type and shape.\n",
    "# --- YOUR CODE HERE ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f4d24",
   "metadata": {},
   "source": [
    "After we got the 4D data matrix. We need to identify which element in the 4D array corresponds to local space coordinates (0. -10, 30).\n",
    "\n",
    "Remember the `affine` matrix in the `func_img` object and how we transformed an element index to the local space coordinates for the T1w image? We can just do it reversely: from local space coordinates (i.e., 0. -10, 3) to the element index in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a83f64",
   "metadata": {},
   "source": [
    "#### 🧠 Do it yourself: \n",
    "Please identify the element index corresponding to local space coordinates (i.e., 0. -10, 3).\n",
    "\n",
    "_Hint: you could use `image.coord_transform` to do this. But remember to pass the **inverse affine matrix** to the command._\n",
    "\n",
    "```python\n",
    "\n",
    "# invert the affine to get the inverse affine matrix\n",
    "inv_affine = np.linalg.inv(func_img.affine)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "_Type your answer in the cell below. then check the answer._\n",
    "\n",
    "<details>\n",
    "<summary>💡 Show the correct answer</summary>\n",
    "\n",
    "````python\n",
    "# invert the affine\n",
    "inv_affine = np.linalg.inv(func_img.affine)\n",
    "\n",
    "# apply the inverse transform: world -> voxel\n",
    "i, j, k = image.coord_transform(0, -10, 3, inv_affine)\n",
    "\n",
    "# as the output i, j, k are not integer\n",
    "# convert them to integer so that they can be used as voxel indices for indexing\n",
    "voxel_idx = np.round([i, j, k]).astype(int)\n",
    "print(\"voxel (i, j, k):\", tuple(voxel_idx))\n",
    "\n",
    "````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e7d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and execute your code below to convert the local space coordinates (0, -10, 3) to the corresponding voxel indices (i, j, k) in the 4D data matrix.\n",
    "# --- YOUR CODE HERE ---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44731fc8",
   "metadata": {},
   "source": [
    "Now with the data matrix and the element index for local space coordinates (0, -10, 3), the rest is straitfoward. You just need to extract the values from the index and plot it. It's purely Python plotting.\n",
    "\n",
    "Below is the complete code of doing this from 4D data extraction, converting local space coordinates to element index, and then extract the data and plot it.\n",
    "You can come up with your version, but make sure you understand each step of the codes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c29f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image data as a numpy array from the Nifti1Image object\n",
    "func_data = func_img.get_fdata()\n",
    "\n",
    "# invert the affine\n",
    "inv_affine = np.linalg.inv(func_img.affine)\n",
    "\n",
    "# apply the inverse transform: world -> voxel\n",
    "i, j, k = image.coord_transform(0, -10, 3, inv_affine)\n",
    "\n",
    "# convert to integer voxel indices for numpy indexing\n",
    "voxel_idx = np.round([i, j, k]).astype(int)\n",
    "\n",
    "# extract the time series at the specified voxel\n",
    "time_series = func_data[tuple(voxel_idx)]\n",
    "\n",
    "# Plot the time series\n",
    "# We use matplotlib for plotting.\n",
    "# see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html for more options\n",
    "plt.plot(time_series)\n",
    "plt.title(f\"Voxel time course at (0, -10, 3) mm (voxel {tuple(voxel_idx)})\")\n",
    "plt.xlabel(\"Time (TRs)\")\n",
    "plt.ylabel(\"Signal intensity\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Session 01",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (fmri-colab)",
   "language": "python",
   "name": "fmri-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

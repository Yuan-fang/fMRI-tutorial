{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1b1e48",
   "metadata": {},
   "source": [
    "# Session 1: Basics of fMRI Preprocessing\n",
    "\n",
    "In this session, we will learn the fundamental concepts and practical steps of **fMRI data preprocessing** — the essential process that prepares raw functional images for further analysis.\n",
    "\n",
    "\n",
    "\n",
    "## Tools We’ll Use\n",
    "\n",
    "### **FSL**\n",
    "We will begin with **[FSL](https://fsl.fmrib.ox.ac.uk/fsl/docs/)** — a comprehensive neuroimaging software library developed at the **University of Oxford**.  \n",
    "FSL provides command-line and GUI tools that let you perform each preprocessing step-by-step. You will gain a concrete understanding of what each stage in preprocessing actually does.\n",
    "\n",
    "### **fMRIPrep**\n",
    "\n",
    "While FSL remains widely used, most modern studies now rely on **[fMRIPrep](https://fmriprep.org/en/stable/)** — an automated, standardized preprocessing pipeline that integrates best-in-class tools from multiple neuroimaging packages, including FSL.\n",
    "\n",
    "### **Why fMRIPrep?**\n",
    "1. **Automation:** With just **one line of code**, you can execute the entire preprocessing workflow.\n",
    "2. **Reproducibility:** Using a standardized pipeline minimizes variability caused by different preprocessing choices across labs and studies.\n",
    "\n",
    "In short, fMRIPrep provides a *robust, reproducible, and community-endorsed* preprocessing workflow — but it’s valuable to first understand what happens under the hood, which is exactly what FSL will help us explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c070d7",
   "metadata": {},
   "source": [
    "Please run the cell below for the runtime of each notebook. It will import necessary packages that you require to go through this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9304ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic Setup (always run this first) ---\n",
    "\n",
    "# Install dependencies \n",
    "%pip install -q gdown\n",
    "%pip install -q git+https://github.com/Yuan-fang/fMRI-tutorial.git\n",
    "\n",
    "# Import essential packages\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import module\n",
    "from nilearn import image, plotting\n",
    "from nilearn.image import index_img\n",
    "from bids import BIDSLayout\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tutorial.utils.paths import PathManager\n",
    "from tutorial.utils.fetch import fetch_dataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Set up data directories ---\n",
    "\n",
    "DATASET = \"Haxby2001\" # name of the dataset\n",
    "BASE_DIR = Path.home() / \"fmri_tutorial\" # base directory for the tutorial\n",
    "DATA_DIR = BASE_DIR / \"data\" / DATASET # data directory for the dataset\n",
    "DERIV_DIR = BASE_DIR / \"data\" / \"derivatives\" # derivatives directory for processed data\n",
    "\n",
    "for p in (DATA_DIR, DERIV_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Download dataset if not already present ---\n",
    "\n",
    "# Google Drive link to the dataset\n",
    "download_url = \"https://drive.google.com/uc?id=1fPjbWhY6ZDOGSm59duKmOcCpgp5Zf5tX\"\n",
    "fetch_dataset(download_url, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab9700",
   "metadata": {},
   "source": [
    "To begin with, We need to specify our raw BIDS data project path, and create a BIDSLayout object to interface with the original BIDS dataset.\n",
    "\n",
    "We will also create a derivative folder where the output files are to be saved.\n",
    "\n",
    "For your own project, you may need to adjust your own paths accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BIDSLayout object to interface with the BIDS dataset\n",
    "layout = BIDSLayout(DATA_DIR, validate=False)  \n",
    "\n",
    "print(\"Data directory:       \", DATA_DIR.resolve())\n",
    "print(\"Derivatives directory:\", DERIV_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c243917e",
   "metadata": {},
   "source": [
    "Let's first get the functional image path of the 1st run of one subject (\"sub-1\").\n",
    "\n",
    "In addition, we also get this subeject's T1w image path from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file path of T1w image for a specific subject (\"sub-1\") \n",
    "anat_path = layout.get(subject=\"1\", suffix=\"T1w\", extension=\".nii.gz\", return_type=\"file\")[0] \n",
    "print(\"Anatomical files for subject 1:\", anat_path)\n",
    "\n",
    "# Get the file path of run 1's functional image for a specific subject (\"sub-1\")\n",
    "func_path = layout.get(subject=\"1\", suffix=\"bold\", extension=\".nii.gz\", run=\"1\", return_type=\"file\")[0]\n",
    "print(\"Functional files for subject 1, run 1:\", func_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe0a71d",
   "metadata": {},
   "source": [
    "### Organizing Output Data in fMRI Analysis\n",
    "\n",
    "When analyzing fMRI data, it’s important to predefine a clean output structure for your results.\n",
    "Otherwise, each preprocessing or modeling step can generate dozens of files across runs, sessions, and subjects, which quickly turn your derivatives folder into chaos.\n",
    "\n",
    "To help you manage this, here we use a custom tool called `PathManager`, which automatically creates and retrieves output paths that follow the BIDS convention.\n",
    "\n",
    "Below is a minimal example showing how to use it.\n",
    "\n",
    "---\n",
    "```python\n",
    "# --- 1. Initialize PathManager ---\n",
    "\n",
    "# BIDSlayout: the BIDSLayout object for your raw BIDS dataset\n",
    "# DERIV_ROOT: the root folder for derivative (output) data\n",
    "# pipeline: your processing pipeline name (e.g. \"fMRIPrep\", \"my_pipeline\", etc.)\n",
    "path_manager = PathManager(BIDSlayout=layout,\n",
    "                           deriv_base=DERIV_DIR,\n",
    "                           pipeline=\"your_fancy_pipeline\")\n",
    "\n",
    "\n",
    "# --- 2. Create a new derivative file path ---\n",
    "\n",
    "# src_file  : the source file (e.g., subject 1's run 1 bold image)\n",
    "# proc      : label for this processing step (e.g., \"smoothed\")\n",
    "# suffix    : data type (e.g., \"bold\")\n",
    "# extension : file format (e.g., \".nii.gz\")\n",
    "output_path = path_manager.create_path(src_file=func_path,\n",
    "                                       proc=\"smoothed\",\n",
    "                                       suffix=\"bold\",\n",
    "                                       ext=\".nii.gz\")\n",
    "\n",
    "\n",
    "# --- 3. Find an existing derivative file ---\n",
    "\n",
    "# Search for a derivative that matches certain entities\n",
    "existing_path = path_manager.find_path(subject=\"1\",\n",
    "                                       run=\"1\",\n",
    "                                       proc=\"smoothed\",\n",
    "                                       suffix=\"bold\",\n",
    "                                       extension=\".nii.gz\")\n",
    "```\n",
    "---\n",
    "Now let's create a PathManager object for our preprocessings with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf747b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PathManager object 'fsl_manager', which is specifically for managing file paths related to FSL processing.\n",
    "fsl_manager = PathManager(\n",
    "    BIDSlayout=layout,\n",
    "    deriv_base=DERIV_DIR,\n",
    "    pipeline=\"fsl_preproc\"\n",
    "    )\n",
    "\n",
    "# Then use the 'manager' object to create the output file path for brain-extracted anatomical image\n",
    "# Here, we specify src (source) as the input anatomical image, \n",
    "# the processing step as \"brain\" (brain extraction) and the suffix as \"T1w\" (T1-weighted image)\n",
    "brain_path = fsl_manager.create_path(src=anat_path, proc=\"brain\", suffix=\"T1w\")\n",
    "print(\"Brain-extracted anatomical image path:\", brain_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f89a3",
   "metadata": {},
   "source": [
    "### Brain Extraction from the T1w Image\n",
    "\n",
    "Now let's apply our first FSL function: **BET**, to our T1w image to extract brain image.\n",
    "\n",
    "> 💡 **Tip:** FSL commands by default are executed in a Shell environment. When running Shell commands (such as FSL commands) inside a Python notebook, prefix the command with an **exclamation mark (`!`)** so that it tells the notebook to execute the command in a **shell environment**, not in Python.\n",
    "> \n",
    "> For example, to check how to use `bet`:\n",
    "> ```bash\n",
    "> !bet -help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25660a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FSL (version 6.0.7.8)\n",
    "# Note other versions may not work with this tutorial\n",
    "await module.load('fsl/6.0.7.8')\n",
    "await module.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44094ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skull-strip the anatomical image using FSL's BET\n",
    "# Note \"{}\" is used to format the string with the variable values in Python so that the anat_path and brain_path are correctly inserted into the BET command.\n",
    "!bet \"{anat_path}\" \"{brain_path}\" -R -f 0.5 -g 0\n",
    "print(f\"Brain-extracted anatomical image created: {brain_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the brain extraction image\n",
    "plotting.plot_anat(brain_path, title=\"Brain-extracted anatomical image\")\n",
    "\n",
    "# Visualize the brain extraction result by overlaying the skull-striped brain on the original anatomical image\n",
    "plotting.plot_roi(brain_path, bg_img=anat_path, alpha=0.3, title=\"BET mask overlay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e818e",
   "metadata": {},
   "source": [
    "### Slice timing correction\n",
    "\n",
    "Slice timing correction adjusts for the fact that, during fMRI acquisition, **different slices of the brain are sampled at slightly different times within each TR**.\n",
    "This correction is more important when precise timing of neural responses matters.\n",
    "\n",
    ">When slice timing is *not necessary*\n",
    ">\n",
    "> For this dataset (a block design), slice timing correction is not needed.  \n",
    "> In block designs, each condition lasts several seconds, and the hemodynamic responses are sustained and overlapping—so small slice-to-slice time offsets have negligible effect on the model fit.  \n",
    "> \n",
    "> Even in event-related designs, slice timing becomes less critical when the TR is short (e.g., ≤ 1 s).  \n",
    "> With such rapid sampling, the maximum inter-slice delay (a few tens of milliseconds) is small relative to the ~5 s width of the HRF.\n",
    "> \n",
    ">---\n",
    ">When slice timing is *important*\n",
    ">\n",
    ">For event-related designs with longer TRs (e.g., 2–3 s), slice timing correction helps align the BOLD signal with stimulus onsets more accurately before modeling.\n",
    "\n",
    "In FSL, slice timing correction is performed using the command-line tool `slicetimer`.\n",
    "You can view its options by running:\n",
    "```bash\n",
    "!slicetimer -help\n",
    "```\n",
    "Note that slice timing correction requires knowing the exact acquisition order of slices.\n",
    "In a BIDS dataset, this information is stored in the JSON sidecar file associated with each functional run (under the \"SliceTiming\" field).\n",
    ">⚠️ **Note:** In this tutorial dataset, JSON files are not available, so the slice timing information cannot be recovered from the data or the original publication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497136ba",
   "metadata": {},
   "source": [
    "### Motion correction\n",
    "\n",
    "Motion correction is the process of **realigning all functional volumes to a reference image (usually the first or middle volume of the run)**.\n",
    "This ensures that each voxel’s time series corresponds to a consistent brain location across the entire scan.\n",
    "\n",
    "Motion correction is typically performed **before** spatial smoothing and temporal filtering.\n",
    "If you applied smoothing first, the signal from moving voxels would be mixed with neighboring tissue, making alignment less accurate and motion artefacts harder to remove.\n",
    "\n",
    "In FSL, motion correction is carried out using the tool `mcflirt` (Motion Correction using FMRIB’s Linear Image Registration Tool).\n",
    "You can view its usage by typing:\n",
    "````bash \n",
    "!mcflirt -help\n",
    "````\n",
    "mcflirt performs rigid-body registration (6 degrees of freedom) to estimate and correct head motion.\n",
    "It can also output transformation matrices and motion parameter files (e.g., *.par), which are often used later as nuisance regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the PathManager object to create the output file path for motion-corrected functional image\n",
    "# Here, we specify the src (source) as the input functional image, \n",
    "# the processing step as \"mc\" (motion correction) and the suffix as \"bold\" (BOLD image)\n",
    "mc_path = fsl_manager.create_path(src=func_path, proc=\"mc\", suffix=\"bold\")\n",
    "\n",
    "# Perform motion correction using FSL's MCFLIRT\n",
    "!mcflirt -in \"{func_path}\" -out \"{mc_path}\" -refvol 0 -plots -report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fff7a8",
   "metadata": {},
   "source": [
    "Click the derivatives folder, you will see two files were automatically created under 'fsl_preproc/sub-1/func/'. One is the motion parameter file and the other is the motion corrected functional image.\n",
    "The motion corrected functional image is the file we just created and saved at 'mc_path'.\n",
    "The motion parameter file is generated by MCFLIRT during the motion correction process, with the extension '.nii.gz.par'.\n",
    "\n",
    "We can get the file path of the motion parameter file and plot the motion parameters for each volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the PathManager to find the motion parameter file generated by MCFLIRT.\n",
    "# The motion parameter file has the extension '.nii.gz.par'.\n",
    "# Here, we specify the subject as '1', processing step as 'mc' (motion correction), run as '1', and extension as '.nii.gz.par'.\n",
    "# The find_path method returns a list of matching file paths, so we take the first element [0].\n",
    "motion_par = fsl_manager.find_path(subject='1', proc='mc', run='1', extension='.nii.gz.par')[0]\n",
    "print(f\"The motion parameter file path: {motion_par}\")\n",
    "\n",
    "# Load motion parameters\n",
    "motion = np.loadtxt(motion_par)\n",
    "\n",
    "# Plot the motion parameters\n",
    "# Create a figure with two subplots: one for translations and one for rotations\n",
    "# Share the x-axis (time/volume)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "# Translations (mm)\n",
    "axes[0].plot(motion[:, 3:]) # Note that the last three columns are translations\n",
    "axes[0].legend([\"x\", \"y\", \"z\"], loc=\"upper right\")\n",
    "axes[0].set_ylabel(\"Translation (mm)\")\n",
    "axes[0].set_title(\"Head translation over time\")\n",
    "\n",
    "# Rotations (radians)\n",
    "axes[1].plot(motion[:, :3]) # Note that the first three columns are rotations\n",
    "axes[1].legend([\"pitch\", \"roll\", \"yaw\"], loc=\"upper right\")\n",
    "axes[1].set_xlabel(\"Volume (TR)\")\n",
    "axes[1].set_ylabel(\"Rotation (radians)\")\n",
    "axes[1].set_title(\"Head rotation over time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cff0cd",
   "metadata": {},
   "source": [
    "The motion parameters estimated by mcflirt (three translations and three rotations) can later be included as nuisance regressors in the general linear model (GLM).\n",
    "Including them helps remove residual signal fluctuations associated with head motion that are not fully corrected by realignment.\n",
    "\n",
    "In practice, researchers often exclude or flag runs where head motion is excessively large, since strong movements can cause signal dropout and spin-history artefacts that cannot be fully corrected.\n",
    "\n",
    "\n",
    "It is generally considered safe to assume that motion is minimal when both:\n",
    ">\n",
    ">Translation (x, y, z displacement) is less than 1 mm, and\n",
    ">\n",
    ">Rotation (pitch, roll, yaw) is less than 1 degree.\n",
    "\n",
    "Runs exceeding these thresholds may still be usable with careful scrubbing or motion modeling, but should be evaluated critically.\n",
    "\n",
    ">⚠️ **Note:** In FSL’s motion parameter files (*.par), rotations are expressed in radians, not degrees.\n",
    "To convert radians to degrees, multiply each rotation value by $$180/\\pi \\approx 57.3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19373faa",
   "metadata": {},
   "source": [
    "### Spatial Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac74677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create smoothed functional image path\n",
    "# processing step is \"smoothed_6mm\" indicating 6mm smoothing\n",
    "smoothed_6mm_path = fsl_manager.create_path(src=func_path, proc=\"smoothed_6mm\", suffix=\"bold\", extension=\".nii.gz\")\n",
    "\n",
    "# Perform spatial smoothing using nilearn's smooth_img function\n",
    "# mc_path is the motion-corrected functional image\n",
    "smoothed_6mm_img = image.smooth_img(mc_path, fwhm=6)  # Apply 6mm FWHM Gaussian smoothing\n",
    "\n",
    "# Save the smoothed image\n",
    "smoothed_6mm_img.to_filename(smoothed_6mm_path)\n",
    "print(f\"Smoothed image saved at: {smoothed_6mm_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the smoothed functional image of one volume (e.g., the 10th volume)\n",
    "plotting.plot_img(index_img(smoothed_6mm_path, 10), title=\"Smoothed functional image (6mm FWHM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7370a63",
   "metadata": {},
   "source": [
    "#### 🤔 Do it yourself: \n",
    "Please smooth the data with the 4mm FWHM and visualize the results\n",
    "\n",
    "_Type your answer in the cell below. then check the answer._\n",
    "\n",
    "<details>\n",
    "<summary>💡 Show the correct answer</summary>\n",
    "\n",
    "````python\n",
    "# Create smoothed functional image path\n",
    "# processing step is \"smoothed_4mm\" indicating 4mm smoothing\n",
    "smoothed_4mm_path = fsl_manager.create_path(src=func_path, proc=\"smoothed_4mm\", suffix=\"bold\", extension=\".nii.gz\")\n",
    "\n",
    "# Perform spatial smoothing using nilearn's smooth_img function\n",
    "# mc_path is the motion-corrected functional image\n",
    "smoothed_4mm_img = image.smooth_img(mc_path, fwhm=4)  \n",
    "\n",
    "# Save the smoothed image\n",
    "smoothed_4mm_img.to_filename(smoothed_4mm_path)\n",
    "print(f\"Smoothed image saved at: {smoothed_4mm_path}\")\n",
    "\n",
    "# Visualize the smoothed functional image of one volume (e.g., the 10th volume)\n",
    "plotting.plot_img(index_img(smoothed_4mm_path, 10), title=\"Smoothed functional image (4mm FWHM)\")\n",
    "\n",
    "````\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7be17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and execute your code below to smooth the functional image with 4mm FWHM\n",
    "# --- YOUR CODE HERE ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68465974",
   "metadata": {},
   "source": [
    "### Temporal filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ddc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sigma values for high-pass and low-pass filtering\n",
    "# 100 seconds/cycle (0.01 Hz) cutoff for high-pass filter\n",
    "hp_sigma = 100 / (2 * np.sqrt(2 * np.log(2)))  # convert 100 seconds to sigma in volumes\n",
    "# we can disable low-pass filter by setting lp_sigma to a negative value\n",
    "# because we only want high-pass filtering here, as the sampling rate is very low\n",
    "lp_sigma = -1 # set <0 to disable low-pass filter\n",
    "\n",
    "# create the output file path for the temporally filtered image\n",
    "filtered_path = fsl_manager.create_path(src=func_path, proc=\"filtered-100s\", suffix=\"bold\", extension=\".nii.gz\")\n",
    "\n",
    "# perform temporal filtering using FSL's fslmaths\n",
    "# smoothed_6mm_path is the input smoothed functional image at 6mm FWHM\n",
    "!fslmaths \"{smoothed_6mm_path}\" -bptf \"{hp_sigma}\" \"{lp_sigma}\"  \"{filtered_path}\" -odt float\n",
    "print(f\"Temporally filtered image saved at: {filtered_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee475781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the before-filtered functional image for comparison\n",
    "before_filtered_func_img = image.load_img(smoothed_6mm_path)\n",
    "\n",
    "# Load the before-filtered image data as a numpy array\n",
    "before_filtered_func_data = before_filtered_func_img.get_fdata()\n",
    "\n",
    "# invert the affine\n",
    "inv_affine = np.linalg.inv(before_filtered_func_img.affine)\n",
    "\n",
    "# apply the inverse transform: world -> voxel\n",
    "i, j, k = image.coord_transform(10, 30, 3, inv_affine)\n",
    "\n",
    "# convert to integer voxel indices for numpy indexing\n",
    "voxel_idx = np.round([i, j, k]).astype(int)\n",
    "\n",
    "# extract the time series at the specified voxel\n",
    "time_series_before_filtered = before_filtered_func_data[tuple(voxel_idx)]\n",
    "\n",
    "# Plot the time series\n",
    "# We use matplotlib for plotting.\n",
    "# see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html for more options\n",
    "plt.plot(time_series_before_filtered, label=\"Before Filtering\")\n",
    "plt.title(\"Voxel time course at (10, 30, 3) mm\")\n",
    "plt.xlabel(\"Time (TRs)\")\n",
    "plt.ylabel(\"Signal intensity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the filtered functional image\n",
    "filtered_func_img = image.load_img(filtered_path)\n",
    "\n",
    "# Load the filtered image data as a numpy array from the Nifti1Image object\n",
    "filtered_func_data = filtered_func_img.get_fdata()\n",
    "\n",
    "# extract the time series at the specified voxel\n",
    "time_series_filtered = filtered_func_data[tuple(voxel_idx)]\n",
    "\n",
    "# Plot the time series\n",
    "# We use matplotlib for plotting.\n",
    "# see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html for more options\n",
    "plt.plot(time_series_filtered, label=\"After Filtering\")\n",
    "plt.title(\"Voxel time course at (10, 30, 3) mm\")\n",
    "plt.xlabel(\"Time (TRs)\")\n",
    "plt.ylabel(\"Signal intensity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600111d",
   "metadata": {},
   "source": [
    "### Registration and Normalization\n",
    "\n",
    "As the final step in preprocessings, we need to bring each run's data in the subject-specific native functional space to a common template space (e.g., MNI). To do this, we ususally do two-step normalization:\n",
    "\n",
    "1. Registration: sub-A's func image **is aligned with** sub-A's T1w image (func -> T1w)\n",
    "2. Normalization: sub-A's T1w image **is normalized to** a common template image (T1w -> MNI) \n",
    "\n",
    "Both steps involve estimating transformations and applying them to the data. \n",
    "\n",
    "Step 1 produces a transformation matrix $ M_{\\text{reg}} $, which aligns the\n",
    "functional image (`func`) to the participant’s anatomical image (`T1w`) by\n",
    "\n",
    "$$\n",
    "\\text{func} \\xrightarrow{\\,*M_{\\text{reg}}\\,} \\text{T1w}.\n",
    "$$\n",
    "\n",
    "Step 2 yields another matrix $ M_{\\text{norm}} $, which brings the anatomical\n",
    "(`T1w`) image into MNI template space:\n",
    "\n",
    "$$\n",
    "\\text{T1w} \\xrightarrow{\\,*M_{\\text{norm}}\\,} \\text{MNI}.\n",
    "$$\n",
    "\n",
    "Combining the two gives the total transformation that maps the functional\n",
    "image directly into MNI space:\n",
    "\n",
    "$$\n",
    "\\text{func} \\xrightarrow{\\,*M_{\\text{total}}\\,} \\text{MNI},\n",
    "\\ where\\\n",
    "M_{\\text{total}} = M_{\\text{reg}}\\,M_{\\text{norm}}.\n",
    "$$\n",
    "\n",
    "Hence, applying $ M_{\\text{total}} $ to the functional data\n",
    "corresponds to first aligning it to `T1w` and then normalizing to MNI:\n",
    "\n",
    "$$\n",
    "\\text{func} * M_{\\text{reg}} * M_{\\text{norm}} = \\text{func} * M_{\\text{total}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f8d21e",
   "metadata": {},
   "source": [
    "#### Register functional to anatomical (func to T1w)\n",
    "To register functional to T1w image, we need to pick one representative frame (volume) from the functional run as the to-be-registered functional frame (or target image). This is often the middle volume or the averaged volume across time. The reference is the same subject's T1w image. \n",
    "\n",
    "The registration produces two important files - one is the registered functional volume, which allow us to evaluate whether the registration is accurate. The other is the transformation matrix, which will be used for final transformation.\n",
    "\n",
    "In FSL, registration can be achieved by `flirt` or `epi_reg` . Here we use `flirt` for practical reasons of saving processing time. `epi_reg` is more accurate but also more computational expensive, besides its performance is best with **field mapping scans**, which we don't have in this dataset.\n",
    "\n",
    "For usages of both functions, type `!flirt --help` and `!epi_reg --help`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output file path for the mean functional image\n",
    "fmean_path = fsl_manager.create_path(src=func_path, proc=\"mean\", suffix=\"bold\", extension=\".nii.gz\")\n",
    "\n",
    "# Create the file (mean across time) by applying FSL's fslmaths with -Tmean option\n",
    "# Note we use the motion-corrected functional image (mc_path) as input here\n",
    "# Because smoothed and filtered images are not sharp enough for registration (registration is done on un-smoothed images as it needs detailed features)\n",
    "!fslmaths {mc_path} -Tmean {fmean_path}\n",
    "print(f\"Mean motion-corrected functional image created: {fmean_path}\")\n",
    "\n",
    "# Register functional to anatomical using epi_reg\n",
    "# Create the output file path for the registered functional image\n",
    "reg_func_path = fsl_manager.create_path(src=func_path, proc=\"reg2highres\", suffix=\"bold\", extension=\".nii.gz\")\n",
    "\n",
    "# Create the output file path for the transformation matrix\n",
    "trans_mat_path = fsl_manager.create_path(src=func_path, proc=\"reg2highres\", suffix=\"bold\", extension=\".mat\")\n",
    "\n",
    "# Perform registration using FSL's flirt\n",
    "# dof 12 indicates 12 degrees of freedom (affine transformation including translation, rotation, scaling, and shearing)\n",
    "!flirt -in \"{fmean_path}\" -ref \"{brain_path}\" -out \"{reg_func_path}\" -omat \"{trans_mat_path}\" -dof 12\n",
    "print(f\"Registered functional image created: {reg_func_path}\")\n",
    "print(f\"Transformation matrix created: {trans_mat_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7b9a2",
   "metadata": {},
   "source": [
    "Let's now overlap the registered mean functional image and T1w image to see how well the registration did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b29d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean functional image with T1 edges\n",
    "display = plotting.plot_epi(reg_func_path, title=\"Mean EPI + T1 edges\")\n",
    "display.add_edges(brain_path)   # white edges by default\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3678e6e3",
   "metadata": {},
   "source": [
    "Look at the edge of T1w image, it corresponds with the corregistered mean functional image very well overally.\n",
    "\n",
    "We can also look the overlap interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac55c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive 3D view of the coregistered mean functional image overlaid on the brain-extracted anatomical image\n",
    "view = plotting.view_img(\n",
    "    reg_func_path,      # coregistered mean functional image\n",
    "    bg_img=brain_path,     # T1 as background\n",
    "    opacity=0.5,           # transparency of EPI overlay\n",
    "    cmap=\"cold_hot\",       # nice diverging colormap for EPI\n",
    ")\n",
    "\n",
    "view  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527647b",
   "metadata": {},
   "source": [
    "#### Normalizing T1w to MNI template\n",
    "Normalization in essense serves the same purpose as the previous regisration step. The reason why it's termed as normalization is that it's no longer alignment between different imaging modalities *within a subject*, but the alignment between an idiosyncratic individual's T1w image to a common template. For that reason, it often involves nonlinear wrapping to bring subject's T1w image as close to the template as possible.\n",
    "\n",
    "To do normalization, of course we will need a template image (e.g., MNI152) at a specific resolution (e.g., 2mm) as the reference image. For better accuracy, people often first do a linear pre-alignment between target and reference images to minimize the gross spatial difference, which is then followed by a nonlinear warpping to fine tune regional difference.\n",
    "\n",
    "In FSL, the linear pre-alignment is done with `Flirt`, the nonlinear warp is done with `fnirt` (FMRIB’s Non-linear Image Registration Tool). Check usage of `fnirt` with `!fnirt --help`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5dedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to fetch an MNI template image from FSL\n",
    "template_path = !echo $FSLDIR/data/standard/MNI152_T1_2mm_brain.nii.gz\n",
    "template_path = template_path[0]\n",
    "\n",
    "# Check the template image info\n",
    "# fslinfo provides information about the NIfTI image, including dimensions, voxel size, data type, etc.\n",
    "!fslinfo \"{template_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will perform linear pre-alignment of the subject's T1w image to the MNI template\n",
    "\n",
    "# Create the pre-alignment transformation matrix path\n",
    "prealign_mat_path = fsl_manager.create_path(src=anat_path, proc=\"highres2MNI\", suffix=\"T1w\", extension=\".mat\")\n",
    "\n",
    "# Perform linear pre-alignment using FSL's flirt\n",
    "# Note by default, flirt uses 12 degrees of freedom for inter-modal registration\n",
    "# We want the pre-alignment matrix only here, so we don't output the aligned image\n",
    "!flirt -in \"{brain_path}\" -ref \"{template_path}\" -omat \"{prealign_mat_path}\"\n",
    "print(f\"Pre-alignment matrix created: {prealign_mat_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will perform nonlinear normalization of the subject's T1w image to the MNI template\n",
    "\n",
    "# Create the output file path for the normalized T1w image\n",
    "norm_anat_path = fsl_manager.create_path(src=anat_path, proc=\"highres2MNI_nonlinear\", suffix=\"T1w\", extension=\".nii.gz\")\n",
    "\n",
    "# Create the output file path for the warp coefficient image\n",
    "# Nonlinear warping produces warp coefficient files that describe the deformation field\n",
    "# It's functionally similar to the transformation matrix in linear registration\n",
    "warp_coef_path = fsl_manager.create_path(src=anat_path, proc=\"highres2MNI_nonlinear\", suffix=\"T1w_warpcoef\", extension=\".nii.gz\")\n",
    "\n",
    "# Perform nonlinear normalization using FSL's fnirt\n",
    "# Note that we use the pre-alignment matrix as an initial affine transformation\n",
    "!fnirt --in=\"{brain_path}\" --aff=\"{prealign_mat_path}\" --ref=\"{template_path}\" --cout=\"{warp_coef_path}\" --iout=\"{norm_anat_path}\"\n",
    "print(f\"Normalized anatomical image created: {norm_anat_path}\")\n",
    "print(f\"Warp coefficient image created: {warp_coef_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbca761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the normalized anatomical image with MNI template edges to confirm how well the normalization did\n",
    "display = plotting.plot_anat(norm_anat_path, title=\"Normalized T1w + MNI edges\")\n",
    "display.add_edges(template_path)   # white edges by default\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e55f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will combine the transformations to bring the functional image to MNI space\n",
    "\n",
    "# Create the output file path for the normalized functional image\n",
    "norm_func_path = fsl_manager.create_path(src=func_path, proc=\"clean\", suffix=\"bold_MNI\", extension=\".nii.gz\")\n",
    "\n",
    "# Perform the transformation using FSL's applywarp\n",
    "# We combine the functional-to-T1w linear transformation matrix and T1w-to-MNI nonlinear warp coefficient\n",
    "# Note we use the temporally filtered functional image (filtered_path) as input here as we just applied the already computed transformations to it\n",
    "!applywarp --in=\"{filtered_path}\" --ref=\"{template_path}\" --out=\"{norm_func_path}\" --warp=\"{warp_coef_path}\" --premat=\"{trans_mat_path}\"\n",
    "print(f\"Normalized functional image created: {norm_func_path}\")\n",
    "\n",
    "# To this point, we have completed the main preprocessing steps:\n",
    "# 1) Brain extraction\n",
    "# 2) Motion correction\n",
    "# 3) Spatial smoothing\n",
    "# 4) Temporal filtering\n",
    "# 5) Registration of functional to anatomical\n",
    "# 6) Normalization of anatomical to MNI space\n",
    "# 7) Applying combined transformations to bring functional to MNI space\n",
    "# The final preprocessed functional image is saved at 'norm_func_path', which is in MNI space and ready for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965a2ca",
   "metadata": {},
   "source": [
    "### fMRIPrep\n",
    "\n",
    "While we have learned about the basic preprocessing steps in preprocessing fMRI data, you may have realized that the steps are tedious and succeptible to data analysts' arbitary choices. We could also use fMRIPrep, a ready-to-use automatic optimizied preprocessing pipeline to do preprocessings for most fMRI data. \n",
    "\n",
    "To use fMRIprep, the data must be in BIDS format. It's also important to bear in mind that: an automatic and common pipeline also have its cons. It could produce suboptimal preprocessing results in unknown scenerios, such as for some data with special scanning sequences. So it's always recommended to know beforehands if your data is suitable for fMRIPrep and always inspect the preprocessing results from it, especially the registration and normalization.\n",
    "\n",
    "Now let's see how preprocessing with fMRIPrep is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849056b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fmriprep\n",
    "await module.load('fmriprep')      \n",
    "await module.list()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203004db",
   "metadata": {},
   "source": [
    "You can check the usage of fMRIprep by typing:\n",
    "`!fmriprep --help`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4412868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fmriprep derivatives directory specified for fMRIPrep outputs\n",
    "DERIV_DIR_fmriprep = BASE_DIR / \"data\" / \"derivatives_fmriprep\"\n",
    "DERIV_DIR_fmriprep.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- The following license, environment variables and paths need to be set for fMRIPrep ----\n",
    "\n",
    "# specify the freesurfer license file path\n",
    "license_path = DATA_DIR / \"license.txt\"\n",
    "os.environ['FS_LICENSE'] = str(license_path)\n",
    "os.environ['APPTAINERENV_FS_LICENSE'] = license_path  # Pass to container\n",
    "\n",
    "# Set up environment variables before running fMRIPrep\n",
    "# These environment variables help fMRIPrep locate necessary resources and optimize performance\n",
    "os.environ['SUBJECTS_DIR'] = f'{DERIV_DIR_fmriprep}/freesurfer' # specify FreeSurfer subjects directory within fmriprep derivatives\n",
    "os.environ['APPTAINERENV_SUBJECTS_DIR'] = os.environ['SUBJECTS_DIR'] # Pass to container\n",
    "os.environ['ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS'] = '6' # limit ITK threads to 6\n",
    "os.environ['MPLCONFIGDIR'] = os.path.expanduser('~/matplotlib-mpldir') # set matplotlib config directory to avoid permission issues\n",
    "\n",
    "# Create the freesurfer directory\n",
    "!mkdir -p {DERIV_DIR_fmriprep}/freesurfer\n",
    "\n",
    "# ---- Run fMRIPrep ----\n",
    "\n",
    "# DATA_DIR: input BIDS dataset directory\n",
    "# DERIV_DIR: output derivatives directory\n",
    "# participant: run for participant level (can also run for group level, if given \"group\" instead. Group level requires all subjects to be preprocessed first and it will give a summary report)\n",
    "# --participant-label 1: specify subject 1 (BIDS ID: \"sub-1\", if two subjects, use \"--participant-label 1 2\" to specify both)\n",
    "# --nprocs 6 --mem 10000: allocate 6 processors and 10GB memory (adjust based on your system resources)\n",
    "# --output-spaces MNI152NLin2009cAsym: specify output spaces (we just want MNI space here)\n",
    "# --fs-no-reconall: skip FreeSurfer's recon-all step (this requires a lot of time and resources)\n",
    "# --skip_bids_validation: skip BIDS validation step\n",
    "# -v: verbose output\n",
    "!fmriprep \\\n",
    "  \"{DATA_DIR}\" \\\n",
    "  \"{DERIV_DIR_fmriprep}\" \\\n",
    "  participant \\\n",
    "  --participant-label 1 \\\n",
    "  --nprocs 6 --mem 10000 \\\n",
    "  --output-spaces MNI152NLin2009cAsym \\\n",
    "  --fs-no-reconall \\\n",
    "  --skip_bids_validation \\\n",
    "  -v\n",
    "\n",
    "print(\"fMRIPrep completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Session 02",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

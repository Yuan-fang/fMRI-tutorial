{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14d730f",
   "metadata": {},
   "source": [
    "# Session 2: General linear model\n",
    "\n",
    "In this session, we will learn how to make a design matrix with different regressors and get the statistical maps.\n",
    "\n",
    "\n",
    "## Tools Weâ€™ll Use\n",
    "\n",
    "### **Nilearn**\n",
    "We will use **[Nilearn](https://nilearn.github.io/stable/index.html)** â€” a comprehensive neuroimaging python package focused specifically on modeling fMRI data (so it does not have too much functionality with preprocessing).\n",
    "\n",
    "Nilearn enables approachable and versatile analyses of brain volumes and surfaces. It provides statistical and machine-learning tools, with instructive documentation & open community.\n",
    "\n",
    "It supports general linear model (GLM) based analysis and leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modeling, classification, decoding, or connectivity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282dfa48",
   "metadata": {},
   "source": [
    "Please run the cell below for the runtime of each notebook. It will import necessary packages that you require to go through this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic Setup (always run this first) ---\n",
    "\n",
    "# Install dependencies \n",
    "%pip install -q gdown\n",
    "%pip install -q git+https://github.com/Yuan-fang/fMRI-tutorial.git\n",
    "\n",
    "# Import essential packages\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "#import module\n",
    "from nilearn import image, plotting\n",
    "from nilearn.image import index_img\n",
    "from nilearn.glm.first_level import glover_hrf, spm_hrf, compute_regressor, FirstLevelModel\n",
    "from bids import BIDSLayout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tutorial.utils.paths import PathManager\n",
    "from tutorial.utils.fetch import fetch_dataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4af52",
   "metadata": {},
   "source": [
    "We also need to set up data directories. You can change to other directories and names according to your preference for your own project. But for this tutorial, let's stick with the same data directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up data directories ---\n",
    "\n",
    "DATASET = \"Haxby2001\" # name of the dataset\n",
    "BASE_DIR = Path(\"../\") # base directory for the tutorial\n",
    "DATA_DIR = BASE_DIR / \"data\" / DATASET # data directory for the dataset\n",
    "DERIV_DIR = BASE_DIR / \"data\" / \"derivatives\" # derivatives directory for processed data\n",
    "\n",
    "for p in (DATA_DIR, DERIV_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print out the data directories\n",
    "print(\"Data directory:       \", DATA_DIR.resolve())\n",
    "print(\"Derivatives directory:\", DERIV_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5809e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset if not already present ---\n",
    "\n",
    "# Google Drive link to the dataset\n",
    "download_url = \"https://drive.google.com/uc?id=1fPjbWhY6ZDOGSm59duKmOcCpgp5Zf5tX\"\n",
    "fetch_dataset(download_url, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8404175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BIDSLayout object to interface with the BIDS dataset\n",
    "layout = BIDSLayout(DATA_DIR, validate=False)  \n",
    "print(f\"BIDS dataset with {len(layout.get_subjects())} subjects loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0826370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file path of T1w image for a specific subject (\"sub-1\") \n",
    "anat_path = layout.get(subject=\"1\", suffix=\"T1w\", extension=\".nii.gz\", return_type=\"file\")[0] \n",
    "print(\"Anatomical files for subject 1:\", anat_path)\n",
    "\n",
    "# Get the file path of run 1's functional image for a specific subject (\"sub-1\")\n",
    "func_path = layout.get(subject=\"1\", suffix=\"bold\", extension=\".nii.gz\", run=\"1\", return_type=\"file\")[0]\n",
    "print(\"Functional files for subject 1, run 1:\", func_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7cf239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PathManager object 'fsl_manager', which is specifically for managing file paths related to FSL processing.\n",
    "fsl_manager = PathManager(\n",
    "    BIDSlayout=layout,\n",
    "    deriv_base=DERIV_DIR,\n",
    "    pipeline=\"fsl_preproc\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa24a15",
   "metadata": {},
   "source": [
    "### GLM on a single run\n",
    "\n",
    "Let's first get the file paths of the data we need for GLM analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file path of the preprocessed functional image for subject \"sub-1\", run \"1\"\n",
    "func_r1_path = fsl_manager.find_path(subject=\"1\", run=\"1\", proc=\"clean\", extension=\".nii.gz\")[0]\n",
    "print(\"Preprocessed functional files for subject 1, run 1:\", func_r1_path)\n",
    "\n",
    "# Get the event timing file for subject \"sub-1\", run \"1\"\n",
    "event_r1_file = layout.get(subject=\"1\", suffix=\"events\", extension=\".tsv\", run=\"1\", return_type=\"file\")[0]\n",
    "print(\"Event timing file for subject 1, run 1:\", event_r1_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f607408",
   "metadata": {},
   "source": [
    "We can load the information from the event timing file and look what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78afe6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the event timing file into a pandas DataFrame\n",
    "events_r1 = pd.read_csv(event_r1_file, sep=\"\\t\")\n",
    "# Display the first 20 rows of the events DataFrame\n",
    "# Note that the unit of 'onset' and 'duration' is in seconds\n",
    "events_r1.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d141e9e",
   "metadata": {},
   "source": [
    "After loading the event timing information, we can visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotting.plot_event(events_r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf40811",
   "metadata": {},
   "source": [
    "#### ðŸ¤” Do it yourself: \n",
    "Please load the event timing information of run 2 of this subject, and visualize its event timing information.\n",
    "\n",
    "_Type your answer in the cell below. then check the answer._\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Show the correct answer</summary>\n",
    "\n",
    "````python\n",
    "# Get the event timing file for subject \"sub-1\", run \"2\"\n",
    "event_r2_file = layout.get(subject=\"1\", suffix=\"events\", extension=\".tsv\", run=\"2\", return_type=\"file\")[0]\n",
    "\n",
    "# Load the event timing file into a pandas DataFrame\n",
    "events_r2 = pd.read_csv(event_r2_file, sep=\"\\t\")\n",
    "\n",
    "\n",
    "# Visualize the event timing information of run 2 of this subject\n",
    "fig = plotting.plot_event(events_r2)\n",
    "\n",
    "````\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and execute your code below to visualize the event timing information of run 2 of this subject.\n",
    "# --- YOUR CODE HERE ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e31a41",
   "metadata": {},
   "source": [
    "### HRF models and HRF-convolved regressor (predictor)\n",
    "\n",
    "Hemodynamic response function (HRF) is a curve that shows how the fMRI signal rises and falls over time after a brief burst of brain activity (or equivalently, a single impulse event). \n",
    "\n",
    "There are many different HRF models in Nilearn. Two most common HRF models are:\n",
    "1. Glover HRF, which is a HRF model based on the double-gamma function proposed by Glover (1999). \n",
    "2. SPM HRF, which is another commonly used HRF model based on the canonical HRF used in SPM software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff47c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we generate the Glover HRF using nilearn's glover_hrf function.\n",
    "# time resolution (TR) = 1s, time length is the length of the HRF to be generated (32s here), onset=0s\n",
    "hrf_glover = glover_hrf(t_r=1, oversampling=1,time_length=32, onset=0)\n",
    "\n",
    "# Plot the glover HRF\n",
    "plt.figure(figsize=(8, 4)) # Set the figure size\n",
    "plt.plot(hrf_glover, label='Glover HRF', color='blue')\n",
    "plt.title('Glover Hemodynamic Response Function (HRF)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the Glover HRF with the SPM HRF\n",
    "hrf_spm = spm_hrf(t_r=1, oversampling=1, time_length=32, onset=0)\n",
    "\n",
    "# Plot both HRFs for comparison\n",
    "plt.figure(figsize=(8, 4)) # Set the figure size\n",
    "plt.plot(hrf_glover, label='Glover HRF', color='blue')   \n",
    "plt.plot(hrf_spm, label='SPM HRF', color='orange')\n",
    "plt.title('Comparison of Glover and SPM Hemodynamic Response Functions (HRFs)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0791956",
   "metadata": {},
   "source": [
    "What differences do you notice of these two models?  \n",
    "\n",
    "Although with those differences, overall, Glover model is similiar to SPM model, and fortunately, using either model indeed does not make a huge difference if what you care is only activations. So we will stick with the Glover model from here onwards.\n",
    "\n",
    "With the HRF model, what we will do next is to **convolve** the HRF to events with specific timings, so that we can get a predictor (regressor) of BOLD response from that event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a602e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Convolution with Glover HRF ------\n",
    "\n",
    "# We need to first create a stimulus to convolve with the HRF\n",
    "# for simplicity, we create an impulse stimulus (a delta function) at time 0, with duration of 1s\n",
    "impulse = np.zeros(100)    # Create an array of zeros with length 100s\n",
    "impulse[0] = 1             # Set the first element to 1 to represent an impulse at time 0 with intensity 1\n",
    "# plot the impulse stimulus\n",
    "plt.figure(figsize=(8, 4)) # Set the figure size\n",
    "plt.stem(impulse, label='Impulse Stimulus')\n",
    "plt.title('Impulse Stimulus')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dfe418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve the impulse with the Glover HRF\n",
    "convolved_response = np.convolve(impulse, hrf_glover)[:100]  # Keep only the first 100s of the convolved response\n",
    "# Plot the convolved response\n",
    "plt.figure(figsize=(10, 4)) # Set the figure size\n",
    "plt.plot(convolved_response, label='Convolved Response', color='green')\n",
    "plt.title('Convolved Response of Impulse with Glover HRF')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e8544",
   "metadata": {},
   "source": [
    "You can try to adjust the duration of the stimulus by yourself, to see its effect on the convolved response. Here, we adjust the stimulus series to make it has more random pulse stimuli, and see what the predicted convolved response will look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ee6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# still we have a volume of zeros with length 100s\n",
    "impulse = np.zeros(100)    # Create an array of zeros with length 100s\n",
    "# set 50 random impulses in the stimulus\n",
    "random_times = np.random.choice(np.arange(100), size=50, replace=False)\n",
    "impulse[random_times] = 1\n",
    "# plot the impulse stimulus\n",
    "plt.figure(figsize=(8, 4)) # Set the figure size\n",
    "plt.stem(impulse, label='Impulse Stimulus')\n",
    "plt.title('Impulse Stimulus')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve the impulse with the Glover HRF\n",
    "convolved_response = np.convolve(impulse, hrf_glover)[:100]  # Keep only the first 100s of the convolved response\n",
    "# Plot the convolved response\n",
    "plt.figure(figsize=(10, 4)) # Set the figure size\n",
    "plt.plot(convolved_response, label='Convolved Response', color='green')\n",
    "plt.title('Convolved Response of Impulse with Glover HRF')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e7735",
   "metadata": {},
   "source": [
    "The event timing file in each run contains the 'onset' and 'duration' information for each type of stimulus (conditions), which in essence gives us information about the timing of stimulus series. As one run always contains 2 or more than 2 conditions (sometimes > 8 conditions if in fast event-related designs), we just need to split the whole timing info into those for each condition and do the convolution seperately for each condition. Nicely, Nilearn provides us a convenient tool to do this: `compute_regressor`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the events for a specific condition: 'house'\n",
    "house_events = events_r1[events_r1['trial_type'] == 'house']\n",
    "print(house_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time frame parameter\n",
    "tr = 2.5  # repetition time in seconds\n",
    "n_scans = 121 # number of scans in the functional run\n",
    "frame_times = np.arange(n_scans) * tr  # frame times in seconds\n",
    "\n",
    "# define the condition events for 'face' condition\n",
    "condition_events = np.array([\n",
    "    house_events['onset'].values,\n",
    "    house_events['duration'].values,\n",
    "    np.ones(len(house_events))  # amplitude is set to 1 for all events\n",
    "])\n",
    "\n",
    "# Now we can use the `compute_regressor` function from nilearn to generate the regressor for this condition\n",
    "regressor, _ = compute_regressor(\n",
    "    exp_condition=condition_events,\n",
    "    frame_times=frame_times,\n",
    "    hrf_model='glover',\n",
    "    con_id='face'\n",
    ")\n",
    "\n",
    "# Plot the generated regressor\n",
    "plt.figure(figsize=(10, 4)) # Set the figure size\n",
    "plt.plot(regressor, label='Face Regressor', color='purple')\n",
    "plt.title('Generated Regressor for House Condition using Glover HRF')\n",
    "plt.xlabel('Time (s)')  \n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422acb9d",
   "metadata": {},
   "source": [
    "We've known that house images evoke responses in the parahippocampal place area (PPA). In MNI152 template space, MNI coordinates (27, -53, -8) is in PPA's range.\n",
    "\n",
    "#### ðŸ¤” Do it yourself: \n",
    "Please plot the fMRI timecourse from preprocessed run 1 at the voxel with MNI coordinates (27, -53, -8). \n",
    "\n",
    "_Type your answer in the cell below. then check the answer._\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Show the correct answer</summary>\n",
    "\n",
    "````python\n",
    "# Load the filtered functional image\n",
    "filtered_func_img = image.load_img(func_r1_path)\n",
    "\n",
    "# Load the filtered image data as a numpy array from the Nifti1Image object\n",
    "filtered_func_data = filtered_func_img.get_fdata()\n",
    "\n",
    "# invert the affine\n",
    "inv_affine = np.linalg.inv(filtered_func_img.affine)\n",
    "\n",
    "# apply the inverse transform: world -> voxel\n",
    "i, j, k = image.coord_transform(27, -53, -8, inv_affine)\n",
    "\n",
    "# convert to integer voxel indices for numpy indexing\n",
    "voxel_idx = np.round([i, j, k]).astype(int)\n",
    "\n",
    "# extract the time series at the specified voxel\n",
    "time_series_filtered = filtered_func_data[tuple(voxel_idx)]\n",
    "\n",
    "# Plot the time series\n",
    "# We use matplotlib for plotting.\n",
    "# see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html for more options\n",
    "plt.figure(figsize=(10, 4)) # Set the figure size\n",
    "plt.plot(time_series_filtered, label=\"After Filtering\")\n",
    "plt.title(\"Voxel time course at (27, -53, -8) mm\")\n",
    "plt.xlabel(\"Time (TRs)\")\n",
    "plt.ylabel(\"Signal intensity\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "````\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c67538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and execute your code below to visualize the fMRI timecourse from preprocessed run 1 at the voxel with MNI coordinates (27, -53, -8). \n",
    "# --- YOUR CODE HERE ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed5a78",
   "metadata": {},
   "source": [
    "### Run the GLM\n",
    "\n",
    "With the regressors specified, we'd like to fit the GLM with our actual fMRI response. You can imagine that, with some tedious work, we could write a script to loop over all the voxels in the brain and for each voxel we do a GLM with different regressors. To get those regressors, something similar to above can be done, such as specifying each condition, get their regressors, combining them together to get a design matrix. This is not conceptually challenging, but it would requires lots of coding work.\n",
    "\n",
    "Nicely, we have a very convinient tool `FirstLevelModel` in Nilearn help us to do all those including what we have done above in one shot. For more details of this tool, please refer to Nilearn documentation: https://nilearn.github.io/modules/generated/nilearn.glm.first_level.FirstLevelModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce78278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "fmri_glm = FirstLevelModel(\n",
    "    t_r=2.5,\n",
    "    hrf_model='glover',\n",
    "    drift_model=None,          # Already high-pass filtered\n",
    "    high_pass=None,            # No additional filtering\n",
    "    smoothing_fwhm=None,       # Already smoothed\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "fmri_glm = fmri_glm.fit(func_r1_path, events=events_r1)\n",
    "\n",
    "print(\"GLM fitting completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074b877",
   "metadata": {},
   "source": [
    "That's it! GLM has been done! We've got all the beta coefficients from all conditions in all voxels.\n",
    "\n",
    "Next we need to get activation maps for each condition and also the contrast between different conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activation map for the 'house' condition\n",
    "house_contrast = fmri_glm.compute_contrast('house')\n",
    "\n",
    "# Visualize the activation map for the 'house' condition\n",
    "plotting.plot_stat_map(\n",
    "    house_contrast,\n",
    "    threshold=1.96, # t=1.96 correspond to p = 0.05 (two-tailed) uncorrected\n",
    "    title='Activation Map for House Condition',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67f28f",
   "metadata": {},
   "source": [
    "#### ðŸ¤” Do it yourself: \n",
    "Get the activation for the 'bottle' condition, and plot the activation map at p = 0.05 uncorrected.\n",
    "\n",
    "_Type your answer in the cell below. then check the answer._\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Show the correct answer</summary>\n",
    "\n",
    "````python\n",
    "# Get activation map for the 'bottle' condition\n",
    "bottle_contrast = fmri_glm.compute_contrast('bottle')\n",
    "\n",
    "# Visualize the activation map for the 'bottle' condition\n",
    "plotting.plot_stat_map(\n",
    "    bottle_contrast,\n",
    "    threshold=1.96, # t=1.96 correspond to p = 0.05 (two-tailed) uncorrected\n",
    "    title='Activation Map for Bottle Condition',\n",
    ")\n",
    "\n",
    "````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb510483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and execute your code below to get the activation for the 'bottle' condition, and plot the activation map at p = 0.05 (two-tailed) uncorrected.\n",
    "# --- YOUR CODE HERE ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6705fc5",
   "metadata": {},
   "source": [
    "For a single condition, the above map tells how the effect size of this condition compared with baseline, or grand mean of a voxel.\n",
    "\n",
    "We could also look at the *difference* or *constrast* between two conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activation map for the 'bottle' condition\n",
    "bottle_contrast = fmri_glm.compute_contrast('bottle')\n",
    "\n",
    "# Get contrast map between 'house' and 'bottle' conditions\n",
    "house_vs_bottle_contrast = fmri_glm.compute_contrast('house - bottle')\n",
    "\n",
    "# Visualize the contrast map between 'house' and 'bottle' conditions\n",
    "plotting.plot_stat_map(\n",
    "    house_vs_bottle_contrast,\n",
    "    threshold=3.1, # z=3.1 correspond to p = 0.001 (1-tailed), uncorrected\n",
    "    title='Contrast Map: House vs Bottle'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bfac10",
   "metadata": {},
   "source": [
    "You can adjust the threshold above to see how threshold can impact the contrast map you got."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02778f2b",
   "metadata": {},
   "source": [
    "#### GLM with regressors of no interest\n",
    "\n",
    "We could also easily add regressors of no interest to our design matrix. For example, those regressors could be head motion parameters, respirotary signals, or errorneous responses made by participants.\n",
    "\n",
    "Here for the sake of simplicity, we only include head motion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get head motion parameters file for subject \"sub-1\", run \"1\"\n",
    "motion_file = fsl_manager.find_path(subject=\"1\", run=\"1\", proc=\"mc\", extension=\".nii.gz.par\")[0]\n",
    "print(\"Head motion parameters file for subject 1, run 1:\", motion_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a83bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the head motion parameters\n",
    "# Note the motion parameters file is a text file with six columns (3 translations and 3 rotations) separated by whitespace\n",
    "# For more information, see the pandas read_csv function documentation\n",
    "motion_params = pd.read_csv(motion_file, delim_whitespace=True, header=None)\n",
    "# Display the first 10 rows\n",
    "motion_params.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include motion parameters as regressors of no interest in the GLM\n",
    "# Initiate a new FirstLevelModel instance with a different name\n",
    "fmri_glm_with_motion = FirstLevelModel(\n",
    "    t_r=2.5,\n",
    "    hrf_model='glover',\n",
    "    drift_model=None,          # Already high-pass filtered\n",
    "    high_pass=None,            # No additional filtering\n",
    "    smoothing_fwhm=None,       # Already smoothed\n",
    ") \n",
    "\n",
    "# Model fitting with motion parameters as additional regressors\n",
    "fmri_glm_with_motion = fmri_glm_with_motion.fit(\n",
    "    func_r1_path, \n",
    "    events=events_r1,\n",
    "    confounds=motion_params  # Pass motion parameters as confounds\n",
    ")\n",
    "print(\"GLM fitting with motion parameters completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffaf41b",
   "metadata": {},
   "source": [
    "Let's also look at the contrast between 'house' vs. 'bottle' to see whether including head motion regressors improved our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activation map for the 'house' condition\n",
    "house_contrast = fmri_glm_with_motion.compute_contrast('house')\n",
    "\n",
    "# Get activation map for the 'bottle' condition\n",
    "bottle_contrast = fmri_glm_with_motion.compute_contrast('bottle')\n",
    "\n",
    "# Get contrast map between 'house' and 'bottle' conditions\n",
    "house_vs_bottle_contrast = fmri_glm_with_motion.compute_contrast('house - bottle')\n",
    "\n",
    "# Visualize the contrast map between 'house' and 'bottle' conditions\n",
    "plotting.plot_stat_map(\n",
    "    house_vs_bottle_contrast,\n",
    "    threshold=3.1, # z=3.1 correspond to p = 0.001 (1-tailed), uncorrected\n",
    "    cut_coords=[20, -56, 3], # to make it comparable to previous contrast map with same cut coords\n",
    "    vmin=-7.2, vmax=7.2,   # to make it comparable to previous contrast map with same color scale\n",
    "    title='Contrast Map: House vs Bottle (with Motion Regressors)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc0539",
   "metadata": {},
   "source": [
    "I hope you can see that after adding motion regressors, the clusters surviving the threshold become larger. Note this is just one run, and this subject's head motion in this run is pretty small, and the effect we are looking here is a very robust one. If you have multiple runs, and head motion is worse and you are looking at some small effect, you probably would have observed the difference between \"no effect\" and \"there is an effect\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadea177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Save the statistic maps -------\n",
    "\n",
    "# Define output directory for saving the statistic maps\n",
    "house_vs_bottle_contrast_path = fsl_manager.create_path(\n",
    "    src=func_r1_path,\n",
    "    proc=\"house-bottle\",\n",
    "    suffix=\"zstat\",\n",
    "    extension=\".nii.gz\"\n",
    ")\n",
    "\n",
    "# Save the contrast map to the defined path\n",
    "house_vs_bottle_contrast.to_filename(house_vs_bottle_contrast_path)\n",
    "print(\"Contrast map saved to:\", house_vs_bottle_contrast_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Session 03",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (fmri-colab)",
   "language": "python",
   "name": "fmri-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
